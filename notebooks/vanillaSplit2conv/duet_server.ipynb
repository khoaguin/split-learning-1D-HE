{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECG Split 1D-CNN Server Side\n",
        "This code is the server part of ECG split 1D-CNN model for **single** client and a server."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required packages, connect to Duet"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, SGD\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "import syft as sy\n",
        "\n",
        "project_path = Path.cwd().parent\n",
        "print(f'project_path: {project_path}')\n",
        "\n",
        "# duet = sy.join_duet(loopback=True)\n",
        "duet = sy.join_duet(target_id='332b215d80ccbd33aeedfc2ce640ba00')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "project_path: /mnt/batch/tasks/shared/LS_root/mounts/clusters/splitlearningcomputer/code/Users/dkn.work/split-learning-1D-HE\nðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n\nâ™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\nâ™«â™«â™« > Use at your own risk.\n\u001b[0m\n\u001b[1m\n    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n    > https://github.com/sponsors/OpenMined\u001b[1m\n\nâ™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\nâ™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\nâ™«â™«â™« >\nâ™«â™«â™« > ...waiting for response from OpenGrid Network... \nâ™«â™«â™« > \u001b[92mDONE!\u001b[0m\n\nâ™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the following Duet Client ID to your duet partner!\nâ™«â™«â™« > Duet Client ID: \u001b[1m014ec143ef1ea473f0643d7a0bcb03a4\u001b[0m\n\nâ™«â™«â™« > ...waiting for partner to connect...\n\nâ™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1630658702537
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the pointers to the data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duet.store.pandas"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "                                        ID       Tags           Description  \\\n0  <UID: 21617bfc219b45b2a515d96d486b9741>  [x_train]   training input data   \n1  <UID: 8d5ca6451cfd44f28d08d509da157840>  [y_train]  training output data   \n2  <UID: 67a45a6773764807b0fa32b4c9d58e9e>   [x_test]    testing input data   \n3  <UID: cee66997a38c41899b3287a31ea1439b>   [y_test]   testing output data   \n\n              object_type  \n0  <class 'torch.Tensor'>  \n1  <class 'torch.Tensor'>  \n2  <class 'torch.Tensor'>  \n3  <class 'torch.Tensor'>  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Tags</th>\n      <th>Description</th>\n      <th>object_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;UID: 21617bfc219b45b2a515d96d486b9741&gt;</td>\n      <td>[x_train]</td>\n      <td>training input data</td>\n      <td>&lt;class 'torch.Tensor'&gt;</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;UID: 8d5ca6451cfd44f28d08d509da157840&gt;</td>\n      <td>[y_train]</td>\n      <td>training output data</td>\n      <td>&lt;class 'torch.Tensor'&gt;</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;UID: 67a45a6773764807b0fa32b4c9d58e9e&gt;</td>\n      <td>[x_test]</td>\n      <td>testing input data</td>\n      <td>&lt;class 'torch.Tensor'&gt;</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;UID: cee66997a38c41899b3287a31ea1439b&gt;</td>\n      <td>[y_test]</td>\n      <td>testing output data</td>\n      <td>&lt;class 'torch.Tensor'&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658716070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ptr = duet.store[\"x_train\"]\n",
        "y_train_ptr = duet.store[\"y_train\"]\n",
        "x_test_ptr = duet.store[\"x_test\"]\n",
        "y_test_ptr = duet.store[\"y_test\"]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658719646
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the user uses  `duet.requests.add_handler(action=\"accept\")` to accept all requests,\n",
        "the server can just get the data without submitting a request everytime"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f'y_train: {y_train_ptr.get_copy()}')\n",
        "except Exception as e:\n",
        "    y_train_ptr.request(reason=\"y_train request\")\n",
        "    print(e)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "y_train: tensor([2, 4, 2,  ..., 1, 4, 0], device='cpu')\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658731503
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data loader pointers for remote training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32  # used for training\n",
        "xdl_ptr = duet.torch.utils.data.DataLoader(x_train_ptr, batch_size=batch_size)\n",
        "ydl_ptr = duet.torch.utils.data.DataLoader(y_train_ptr, batch_size=batch_size)\n",
        "xtdl_ptr = duet.torch.utils.data.DataLoader(x_test_ptr, batch_size=batch_size)\n",
        "ytdl_ptr = duet.torch.utils.data.DataLoader(y_test_ptr, batch_size=batch_size)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658734083
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xdl_ptr"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<syft.proxy.torch.utils.data.DataLoaderPointer at 0x7fbac5869520>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658172140
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define ECG 1D CNN model for both the client and server sides\n",
        "Client side has **2 convolution layers**.\n",
        "Server side has **2 fully connected layers**.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class EcgClient(sy.Module):\n",
        "    def __init__(self, torch_ref):\n",
        "        super(EcgClient, self).__init__(torch_ref=torch_ref)\n",
        "        self.conv1 = self.torch_ref.nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
        "        self.relu1 = self.torch_ref.nn.LeakyReLU()\n",
        "        self.pool1 = self.torch_ref.nn.MaxPool1d(2)  # 64 x 16\n",
        "        self.conv2 = self.torch_ref.nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
        "        self.relu2 = self.torch_ref.nn.LeakyReLU()\n",
        "        self.pool2 = self.torch_ref.nn.MaxPool1d(2)  # 32 x 16\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 32 * 16)\n",
        "        return x\n",
        "\n",
        "class EcgServer(sy.Module):\n",
        "    def __init__(self, torch_ref):\n",
        "        super(EcgServer, self).__init__(torch_ref=torch_ref)\n",
        "        self.linear3 = nn.Linear(32 * 16, 128)\n",
        "        self.relu3 = nn.LeakyReLU() \n",
        "        self.linear4 = nn.Linear(128, 5)\n",
        "        self.softmax4 = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.linear4(x)\n",
        "        x = self.softmax4(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1630658745196
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set random seed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 69  # the meaning of life\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1630658747623
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_torch = duet.torch\n",
        "remote_torch.manual_seed(seed)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "<syft.proxy.torch.GeneratorPointer at 0x7f0f1a333310>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658749561
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make the models and assign intial weight as same as non-split model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ecg_client = EcgClient(torch_ref=torch)\n",
        "checkpoint = torch.load(\"init_weight.pth\")\n",
        "ecg_client.conv1.weight.data = checkpoint[\"conv1.weight\"]\n",
        "ecg_client.conv1.bias.data = checkpoint[\"conv1.bias\"]\n",
        "ecg_client.conv2.weight.data = checkpoint[\"conv2.weight\"]\n",
        "ecg_client.conv2.bias.data = checkpoint[\"conv2.bias\"]\n",
        "\n",
        "ecg_server = EcgServer(torch_ref=torch)\n",
        "checkpoint = torch.load(\"init_weight.pth\")\n",
        "ecg_server.linear3.weight.data = checkpoint[\"linear3.weight\"]\n",
        "ecg_server.linear3.bias.data = checkpoint[\"linear3.bias\"]\n",
        "ecg_server.linear4.weight.data = checkpoint[\"linear4.weight\"]\n",
        "ecg_server.linear4.bias.data = checkpoint[\"linear4.bias\"]"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658975779
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send the server's model to cuda, the client model's pointer to duet. The client trains on CPU"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "server_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"server's device: {torch.cuda.get_device_name(0)}\")\n",
        "ecg_server.cuda(server_device)\n",
        "\n",
        "ecg_client_ptr = ecg_client.send(duet)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "server's device: Tesla K80\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658978912
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets ask the Data Owner if his Machine has CUDA, then put the model on CUDA if he has it"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client_cuda = False\n",
        "client_cuda_ptr = remote_torch.cuda.is_available()\n",
        "client_cuda = bool(client_cuda_ptr.get(\n",
        "                    request_block=True,\n",
        "                    reason=\"To run test and inference locally\",\n",
        "                    timeout_secs=5,  # change to something slower\n",
        "))\n",
        "print(f'client has CUDA? {client_cuda}')\n",
        "\n",
        "client_device_ptr = remote_torch.device(\"cuda\" if client_cuda else \"cpu\")\n",
        "print(f'client device: {client_device_ptr.type.get_copy()}')\n",
        "# if we have CUDA lets send our model to the GPU\n",
        "if client_cuda:\n",
        "    ecg_client_ptr.cuda(client_device_ptr)\n",
        "else:\n",
        "    ecg_client_ptr.cpu()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "client has CUDA? True\nclient device: cuda\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658781526
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check again the initial weights of the created ecg_client and the remote model after sending to duet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = ecg_client_ptr.conv1.get_copy()\n",
        "torch.equal(a.weight.data, ecg_client.conv1.weight.data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630657012933
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set other hyperparameters in the model\n",
        "Hyperparameters here should be same with the client side."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "total_batch = 414  # 32*414=13248. We have 13245 data samples\n",
        "\n",
        "epoch = 400\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "\n",
        "optim_client = remote_torch.optim.Adam(params=ecg_client_ptr.parameters(), lr=lr)\n",
        "optim_server = torch.optim.Adam(params=ecg_server.parameters(), lr=lr)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1630658821665
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training process"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = list()\n",
        "train_accs = list()\n",
        "test_losses = list()\n",
        "test_accs = list()\n",
        "best_test_acc = 0  # best test accuracy\n",
        "for e in range(epoch):\n",
        "    print(f\"Epoch {e+1} - train \", end='')\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    for i in tqdm(range(total_batch)):\n",
        "        # get the next batch's data\n",
        "        inputs_ptr = next(iter(xdl_ptr))\n",
        "        gt_ptr = next(iter(ydl_ptr))\n",
        "        # initialize all gradients to zero\n",
        "        optim_server.zero_grad()\n",
        "        optim_client.zero_grad()\n",
        "        # compute and get the activation signals from the first half of the network\n",
        "        activations_ptr = ecg_client_ptr(inputs_ptr)\n",
        "        # the server still gets access to plain activation signals\n",
        "        activations = activations_ptr.clone().get(request_block=True)\n",
        "        print(activations.device)\n",
        "        # the server continues the forward pass on the activation maps\n",
        "        # activations_server = activations.to(server_device)  # put on GPU\n",
        "        y_hat = ecg_server(activations)\n",
        "        # the server asks to access ground truths in plain text, then put on GPU\n",
        "        y_gt = gt_ptr.get_copy().to(server_device)\n",
        "        # calculates cross-entropy loss\n",
        "        loss = criterion(y_hat, y_gt)\n",
        "        train_loss += loss.item()\n",
        "        correct += torch.sum(y_hat.argmax(dim=1) == y_gt).item()\n",
        "        # backward propagation (calculating gradients of the loss w.r.t the weights)\n",
        "        loss.backward()\n",
        "        # send the gradients to the client\n",
        "        client_grad_ptr = activations.grad.clone().send(duet)\n",
        "        # update the gradients of the client's model\n",
        "        activations_ptr.backward(client_grad_ptr)\n",
        "        # update the weights based on the gradients\n",
        "        optim_client.step()\n",
        "        optim_server.step()\n",
        "\n",
        "        total += len(y_gt)\n",
        "\n",
        "    train_losses.append(train_loss / total_batch)\n",
        "    train_accs.append(correct / total)\n",
        "\n",
        "    print(f'loss: {train_losses[-1]: .4f}, accuracy: {train_accs[-1]*100: 2f}')\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():  \n",
        "        test_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        for i in tqdm(range(total_batch)):\n",
        "            # get the next batch's data\n",
        "            test_inputs_ptr = next(iter(xtdl_ptr))\n",
        "            test_gt_ptr = next(iter(ytdl_ptr))\n",
        "            # forward pass\n",
        "            activations_ptr = ecg_client_ptr(test_inputs_ptr)\n",
        "            activations = activations_ptr.clone().get(request_block=True)\n",
        "            y_hat = ecg_server(activations.to(server_device))\n",
        "            # the server asks to access ground truths in plain text\n",
        "            y_gt = test_gt_ptr.get_copy().to(server_device)\n",
        "            # calculate test loss\n",
        "            loss = criterion(y_hat, y_gt)\n",
        "            test_loss += loss.item()\n",
        "            correct += torch.sum(y_hat.argmax(dim=1) == y_gt).item()\n",
        "            total += len(y_gt)\n",
        "\n",
        "        test_losses.append(test_loss / total_batch)\n",
        "        test_accs.append(correct / total)\n",
        "        print(f'test_loss: {test_losses[-1]: .4f}, test_acc: {test_accs[-1]*100: 2f}')\n",
        "        \n",
        "    if test_accs[-1] > best_test_acc:\n",
        "        best_test_acc = test_accs[-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "gather": {
          "logged": 1630418638557
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_acc_epoch = np.array(test_accs).argmax() + 1\n",
        "print('Best test accuracy {:.2f}% in epoch {}.'.format(best_test_acc*100, best_test_acc_epoch))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({  # save model training process into csv file\n",
        "        'loss': train_losses,\n",
        "        'test_loss': test_losses,\n",
        "        'acc': train_accs,\n",
        "        'test_acc': test_accs\n",
        "    })\n",
        "df.to_csv(os.path.join('csv', 'loss_and_acc.csv'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join('csv', 'loss_and_acc.csv'))\n",
        "test_accs = df['test_acc']\n",
        "train_accs = df['acc']\n",
        "test_losses = df['test_loss']\n",
        "train_losses = df['loss']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
        "\n",
        "ax[0].plot(train_losses, color='red')\n",
        "ax[0].plot(test_losses, color='green')\n",
        "ax[0].set_xticks([0, 100, 200, 300, 400])\n",
        "ax[0].set_xlabel('Epoch', size=16)\n",
        "ax[0].set_ylabel('Loss', size=16)\n",
        "ax[0].set_ylim(0.9, 1.1)\n",
        "ax[0].set_yticks([0.9, 1.0, 1.1, 1.2])\n",
        "ax[0].grid(alpha=0.5)\n",
        "ax[0].tick_params(labelsize=16)\n",
        "ax[0].legend(['Train', 'Test'], loc='right', fontsize=16)\n",
        "\n",
        "\n",
        "ax[1].set_ylim(0.7, 1.0)\n",
        "ax[1].set_yticks([0.7, 0.8, 0.9, 1.0])\n",
        "ax[1].plot(train_accs, color='red')\n",
        "ax[1].plot(test_accs, color='green')\n",
        "yt = ax[1].get_yticks()\n",
        "ax[1].set_yticklabels(['{:,.0%}'.format(x) for x in yt])\n",
        "ax[1].set_xticks([0, 100, 200, 300, 400])\n",
        "ax[1].set_xlabel('Epoch', size=16)\n",
        "ax[1].set_ylabel('Accuracy', size=16, labelpad=-5)\n",
        "ax[1].grid(alpha=0.5)\n",
        "ax[1].tick_params(labelsize=16)\n",
        "ax[1].legend(['Train', 'Test'], loc='right', fontsize=16)\n",
        "\n",
        "fig.savefig('loss_acc_conv2_split_seed.pdf', bbox_inches='tight')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "duet.close()"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630658630057
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "pygments_lexer": "ipython3",
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "syftenv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "version": 3,
    "kernelspec": {
      "name": "syftenv2",
      "language": "python",
      "display_name": "SyftEnv2"
    },
    "interpreter": {
      "hash": "870a257bdc5888c6c86cab99e489c6f2724cdfe1684d5990b9d7489b2d46bf07"
    },
    "file_extension": ".py",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}