{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/split-learning-1D-HE')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[0]\n",
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    return recvall(sock, msglen)\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('127.0.0.1', 43114)\n"
     ]
    }
   ],
   "source": [
    "host = 'localhost'\n",
    "port = 10080\n",
    "max_recv = 4096\n",
    "\n",
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)\n",
    "conn, addr = s.accept()\n",
    "print('Conntected with', addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model on the server side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgServer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgServer, self).__init__()\n",
    "        self.linear = nn.Linear(256, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EcgServer(\n",
       "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "ecg_server = EcgServer()\n",
    "\n",
    "checkpoint = torch.load(project_path/\"weights/init_weight_256.pth\")\n",
    "ecg_server.linear.weight.data = checkpoint[\"linear.weight\"]\n",
    "ecg_server.linear.bias.data = checkpoint[\"linear.bias\"]\n",
    "\n",
    "ecg_server.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "total_batch = 414\n",
    "\n",
    "epoch = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = Adam(ecg_server.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = list()\n",
    "train_accs = list()\n",
    "test_losses = list()\n",
    "test_accs = list()\n",
    "best_test_acc = 0  # best test accuracy\n",
    "\n",
    "for e in range(epoch):\n",
    "    print(\"Epoch {} - \".format(e+1), end='')\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for i in range(total_batch):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()  # initialize all gradients to zero\n",
    "        msg = recv_msg(conn)  # receive client message from socket\n",
    "        a = pickle.loads(msg)  # deserialize\n",
    "        a = a.to(device)\n",
    "        a2 = ecg_server(a)  # forward propagation\n",
    "        \n",
    "        loss.backward()  # backward propagation\n",
    "        client_grad = client_output.grad.clone().detach()\n",
    "        msg = pickle.dumps(client_grad)\n",
    "        send_msg(conn, msg)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        correct += torch.sum(output.argmax(dim=1) == label).item()\n",
    "        total += len(label)\n",
    "    train_losses.append(train_loss / total_batch)\n",
    "    train_accs.append(correct / total)\n",
    "    train_status = \"loss: {:.4f}, acc: {:.2f}% / \".format(train_losses[-1], train_accs[-1]*100)\n",
    "    print(train_status, end='')\n",
    "        \n",
    "    with torch.no_grad():  # calculate test accuracy\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for j in range(total_batch):\n",
    "            msg = recv_msg(conn)\n",
    "            msg = pickle.loads(msg)\n",
    "            client_output = msg['client_output']\n",
    "            test_label = msg['label']\n",
    "            client_output, test_label = client_output.to(device), test_label.to(device)\n",
    "            test_output = ecg_server(client_output)\n",
    "            loss = criterion(test_output, test_label)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            correct += torch.sum(test_output.argmax(dim=1) == test_label).item()\n",
    "            total += len(test_label)\n",
    "        test_losses.append(test_loss / total_batch)\n",
    "        test_accs.append(correct / total)\n",
    "        test_status = \"test_loss: {:.4f}, test_acc: {:.2f}%\".format(test_losses[-1], test_accs[-1]*100)\n",
    "        print(test_status)\n",
    "        \n",
    "    if test_accs[-1] > best_test_acc:\n",
    "        best_test_acc = test_accs[-1]\n",
    "    \n",
    "    msg = pickle.dumps(train_status + test_status)\n",
    "    send_msg(conn, msg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fb056718432581df00ec0ea1d8474ad391dc3f52021963853b762465b19daea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('privsecai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
