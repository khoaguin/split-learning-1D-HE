{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from icecream import ic\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import syft as sy\n",
        "from syft.core.node.vm.vm import VirtualMachine\n",
        "from syft.core.node.vm.client import VirtualMachineClient\n",
        "from syft.ast.module import Module\n",
        "from syft.core.remote_dataloader import RemoteDataLoader\n",
        "from syft.core.remote_dataloader import RemoteDataset\n",
        "\n",
        "print(f'torch version: {torch.__version__}')\n",
        "print(f'syft version: {sy.__version__}')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch version: 1.8.1+cu102\nsyft version: 0.5.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1631728123511
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files and Directories"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# paths to files and directories\n",
        "project_path = Path.cwd().parent\n",
        "print(f'project_path: {project_path}')\n",
        "data_dir = 'mitdb'\n",
        "train_name = 'train_ecg.hdf5'\n",
        "test_name = 'test_ecg.hdf5'\n",
        "all_name = 'all_ecg.hdf5'\n",
        "model_dir = 'model'\n",
        "model_name = 'conv2'\n",
        "model_ext = '.pth'\n",
        "csv_dir = 'csv'\n",
        "csv_ext = '.csv'\n",
        "csv_name = 'conv2'\n",
        "csv_accs_name = 'accs_conv2'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "project_path: /mnt/batch/tasks/shared/LS_root/mounts/clusters/teslak80-56gbram/code/Users/dkn.work/split-learning-he\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631731610686
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct the client and server"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "server: VirtualMachine = sy.VirtualMachine(name=\"server\")\n",
        "client: VirtualMachineClient = server.get_root_client()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694990332
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_torch: Module = client.torch\n",
        "remote_torch"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631694990727
        },
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Client: loading and exploring the dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class ECG(Dataset):\n",
        "    # The class used to load the ECG dataset\n",
        "    def __init__(self, mode='train'):\n",
        "        if mode == 'train':\n",
        "            with h5py.File(project_path/data_dir/train_name, 'r') as hdf:\n",
        "                self.x = torch.tensor(hdf['x_train'][:], dtype=torch.float)\n",
        "                self.y = torch.tensor(hdf['y_train'][:])                \n",
        "        elif mode == 'test':\n",
        "            with h5py.File(project_path/data_dir/test_name, 'r') as hdf:\n",
        "                self.x = torch.tensor(hdf['x_test'][:], dtype=torch.float)\n",
        "                self.y = torch.tensor(hdf['y_test'][:])\n",
        "        else:\n",
        "            raise ValueError('Argument of mode should be train or test')\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694991144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ECG(mode='train')\n",
        "test_dataset = ECG(mode='test')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694991589
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first get everything in the dataset and see how many examples we have, and how each\n",
        "of them look like"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
        "x_train, y_train = next(iter(train_loader))\n",
        "x_test, y_test = next(iter(test_loader))\n",
        "print(f'x_train: {type(x_train)}, {x_train.size()}')\n",
        "print(f'y_train: {type(y_train)}, {y_train.size()}')\n",
        "print(f'x_test: {type(x_test)}, {x_test.size()}')\n",
        "print(f'y_test: {type(y_test)}, {y_test.size()}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631694991928
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = x_train[0, :, :]\n",
        "print(f'x_0: {x0.shape}')\n",
        "x0_unroll = x0.view(-1)\n",
        "print(f'unrolling: {x0_unroll.shape}')\n",
        "indx = np.arange(0, 128)\n",
        "\n",
        "# plt.figure(figsize=(3,3))\n",
        "plt.style.use('dark_background')\n",
        "plt.plot(indx, x0_unroll)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631694992353
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The client creates the Dataset object and save it in a `.pt` file. \n",
        "If using `duet`, he can send the string path to the server using \n",
        "`sy.lib.python.String(string_path).send(duet, pointable=True, tags=[\"data\"])`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(train_dataset, \"train_dataset.pt\")\n",
        "torch.save(test_dataset, \"test_dataset.pt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694992743
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Server: creating remote dataset and dataloader"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_rds = RemoteDataset(path='train_dataset.pt', data_type=\"torch_tensor\")\n",
        "train_rds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694993162
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the remote dataset, the server constructs the data loader. Then the server uses `.send`\n",
        "to create a pointer to do remote data loading"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_rdl = RemoteDataLoader(remote_dataset=train_rds, batch_size=32)\n",
        "train_rdl_ptr = train_rdl.send(client)\n",
        "ic(train_rdl, train_rdl_ptr)\n",
        "# call create_dataset to create the real Dataset object on remote side\n",
        "train_rdl_ptr.load_dataset()\n",
        "# call create_dataloader to create the real DataLoader object on remote side\n",
        "train_rdl_ptr.create_dataloader()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694993734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, b in enumerate(tqdm(train_rdl_ptr)):\n",
        "    if i<2:\n",
        "        X, y = b[0], b[1]\n",
        "        ic(X, y)\n",
        "        ic(X.get_copy(), y.get_copy())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694994317
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, for the test dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_rds = RemoteDataset(path='test_dataset.pt', data_type=\"torch_tensor\")\n",
        "test_rdl = RemoteDataLoader(remote_dataset=test_rds, batch_size=32)\n",
        "test_rdl_ptr = test_rdl.send(client)\n",
        "ic(test_rds, test_rdl, test_rdl_ptr)\n",
        "# call create_dataset to create the real Dataset object on remote side\n",
        "test_rdl_ptr.load_dataset()\n",
        "# call create_dataloader to create the real DataLoader object on remote side\n",
        "test_rdl_ptr.create_dataloader()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694994859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, b in enumerate(test_rdl_ptr):\n",
        "    if i<2:\n",
        "        X, y = b[0], b[1]\n",
        "        ic(X, y)\n",
        "        ic(X.get_copy(), y.get_copy())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694995445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Server: define the spit neural network used to train on the ECG dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client's side contains conv layers"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EcgClient(sy.Module):\n",
        "    # used by the data owners\n",
        "    def __init__(self, torch_ref):\n",
        "        super(EcgClient, self).__init__(torch_ref=torch_ref)\n",
        "        self.conv1 = self.torch_ref.nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
        "        self.relu1 = self.torch_ref.nn.LeakyReLU()\n",
        "        self.pool1 = self.torch_ref.nn.MaxPool1d(2)  # 64 x 16\n",
        "        self.conv2 = self.torch_ref.nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
        "        self.relu2 = self.torch_ref.nn.LeakyReLU()\n",
        "        self.pool2 = self.torch_ref.nn.MaxPool1d(2)  # 32 x 16\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 32 * 16)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631694996046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Server's side contains fully connected layers"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EcgServer(sy.Module):\n",
        "    def __init__(self, torch_ref):\n",
        "        super(EcgServer, self).__init__(torch_ref=torch_ref)\n",
        "        self.linear3 = nn.Linear(32 * 16, 128)\n",
        "        self.relu3 = nn.LeakyReLU() \n",
        "        self.linear4 = nn.Linear(128, 5)\n",
        "        self.softmax4 = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.linear4(x)\n",
        "        x = self.softmax4(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631694996384
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Server: training process"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ecg_client = EcgClient(torch_ref=torch)\n",
        "checkpoint = torch.load(\"init_weight.pth\")\n",
        "ecg_client.conv1.weight.data = checkpoint[\"conv1.weight\"]\n",
        "ecg_client.conv1.bias.data = checkpoint[\"conv1.bias\"]\n",
        "ecg_client.conv2.weight.data = checkpoint[\"conv2.weight\"]\n",
        "ecg_client.conv2.bias.data = checkpoint[\"conv2.bias\"]\n",
        "\n",
        "ecg_server = EcgServer(torch_ref=torch)\n",
        "checkpoint = torch.load(\"init_weight.pth\")\n",
        "ecg_server.linear3.weight.data = checkpoint[\"linear3.weight\"]\n",
        "ecg_server.linear3.bias.data = checkpoint[\"linear3.bias\"]\n",
        "ecg_server.linear4.weight.data = checkpoint[\"linear4.weight\"]\n",
        "ecg_server.linear4.bias.data = checkpoint[\"linear4.bias\"]\n",
        "\n",
        "# Send the client's model to the client\n",
        "ecg_client_ptr = ecg_client.send(client)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631694996732
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some hyper-parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = 414  # 32*414=13248. We have 13245 data samples\n",
        "\n",
        "epoch = 400\n",
        "lr = 0.001\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631696489371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim_client = remote_torch.optim.Adam(params=ecg_client_ptr.parameters(), lr=lr)\n",
        "optim_server = torch.optim.Adam(params=ecg_server.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training (with CPU)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = list()\n",
        "train_accs = list()\n",
        "test_losses = list()\n",
        "test_accs = list()\n",
        "best_test_acc = 0  # best test accuracy\n",
        "for e in range(epoch):\n",
        "    print(f\"Epoch {e+1} - train \", end='')\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    for i, batch in enumerate(tqdm(train_rdl_ptr)):\n",
        "        x_ptr, y_gt_ptr = batch[0], batch[1]\n",
        "        # ic(x.get_copy(), y.get_copy())\n",
        "        # initialize all gradients to zero\n",
        "        optim_server.zero_grad()\n",
        "        optim_client.zero_grad()\n",
        "        # compute and get the activation signals from the first half of the network\n",
        "        activs_ptr = ecg_client_ptr(x_ptr)\n",
        "        # the server still gets access to plain activation signals\n",
        "        activs = activs_ptr.clone().get(request_block=True)\n",
        "        # the server continues the forward pass on the activation maps\n",
        "        y_hat = ecg_server(activs)\n",
        "        # the server asks to access ground truths in plain text\n",
        "        y_gt = y_gt_ptr.get_copy()\n",
        "        # calculates cross-entropy loss\n",
        "        loss = criterion(y_hat, y_gt)\n",
        "        train_loss += loss.item()\n",
        "        correct += torch.sum(y_hat.argmax(dim=1) == y_gt).item()\n",
        "        # backward propagation (calculating gradients of the loss w.r.t the weights)\n",
        "        loss.backward()\n",
        "        # send the gradients to the client\n",
        "        client_grad_ptr = activs.grad.clone().send(client)\n",
        "        # update the gradients of the client's model\n",
        "        activs_ptr.backward(client_grad_ptr)\n",
        "        # update the weights based on the gradients\n",
        "        optim_client.step()\n",
        "        optim_server.step()\n",
        "        total += len(y_gt)\n",
        "\n",
        "    train_losses.append(train_loss / total_batch)\n",
        "    train_accs.append(correct / total)\n",
        "\n",
        "    print(f'loss: {train_losses[-1]: .4f}, accuracy: {train_accs[-1]*100: 2f}')\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():  \n",
        "        test_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        for i, batch in enumerate(tqdm(test_rdl_ptr)):\n",
        "            x_ptr, y_gt_ptr = batch[0], batch[1]\n",
        "            # forward pass\n",
        "            activs_ptr = ecg_client_ptr(x_ptr)\n",
        "            activs = activs_ptr.clone().get(request_block=True)\n",
        "            y_hat = ecg_server(activs)\n",
        "            # the server asks to access ground truths in plain text\n",
        "            y_gt = y_gt_ptr.get_copy()\n",
        "            # calculate test loss\n",
        "            loss = criterion(y_hat, y_gt)\n",
        "            test_loss += loss.item()\n",
        "            correct += torch.sum(y_hat.argmax(dim=1) == y_gt).item()\n",
        "            total += len(y_gt)\n",
        "\n",
        "        test_losses.append(test_loss / total_batch)\n",
        "        test_accs.append(correct / total)\n",
        "        print(f'test_loss: {test_losses[-1]: .4f}, test_acc: {test_accs[-1]*100: 2f}')\n",
        "        \n",
        "    if test_accs[-1] > best_test_acc:\n",
        "        best_test_acc = test_accs[-1]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1 - train loss:  1.0215, accuracy:  88.312571\ntest_loss:  1.0128, test_acc:  89.218573\nEpoch 2 - train loss:  1.0201, accuracy:  88.440921\ntest_loss:  1.0171, test_acc:  88.659872\nEpoch 3 - train loss:  1.0183, accuracy:  88.614572\ntest_loss:  1.0114, test_acc:  89.316723\nEpoch 4 - train loss:  1.0171, accuracy:  88.720272\ntest_loss:  1.0086, test_acc:  89.467724\nEpoch 5 - train loss:  1.0164, accuracy:  88.735372\ntest_loss:  1.0079, test_acc:  89.596074\nEpoch 6 - train loss:  1.0137, accuracy:  88.984522\ntest_loss:  1.0134, test_acc:  89.143073\nEpoch 7 - train loss:  1.0136, accuracy:  89.014723\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "  0%|          | 0/414 [00:00<?, ?it/s]/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/lib/torch/uppercase_tensor.py:30: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  grad = getattr(obj, \"grad\", None)\n100%|██████████| 414/414 [01:22<00:00,  5.05it/s]\n100%|██████████| 414/414 [00:58<00:00,  7.09it/s]\n100%|██████████| 414/414 [01:03<00:00,  6.52it/s]\n100%|██████████| 414/414 [00:58<00:00,  7.12it/s]\n100%|██████████| 414/414 [01:03<00:00,  6.54it/s]\n100%|██████████| 414/414 [00:58<00:00,  7.13it/s]\n100%|██████████| 414/414 [01:03<00:00,  6.54it/s]\n100%|██████████| 414/414 [00:57<00:00,  7.14it/s]\n100%|██████████| 414/414 [01:03<00:00,  6.56it/s]\n100%|██████████| 414/414 [00:58<00:00,  7.12it/s]\n100%|██████████| 414/414 [01:11<00:00,  5.83it/s]\n100%|██████████| 414/414 [00:58<00:00,  7.08it/s]\n100%|██████████| 414/414 [01:03<00:00,  6.56it/s]\n 13%|█▎        | 54/414 [00:07<00:51,  7.03it/s]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_19280/4149109902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mx_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mactivs_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mecg_client_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mactivs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivs_ptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_block\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mecg_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/lib/torch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     ) -> Any:\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_19280/3549125120.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/ast/klass.py\u001b[0m in \u001b[0;36mrun_class_method\u001b[0;34m(__self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             )\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0m__self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_immediate_msg_without_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         inherit_tags(\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/node/common/client.py\u001b[0m in \u001b[0;36msend_immediate_msg_without_reply\u001b[0;34m(self, msg, route_index)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m             \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigning_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigning_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"> Sending {msg.pprint} {self.pprint} ➡️  {msg.address.pprint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroutes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroute_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_immediate_msg_without_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(self, signing_key)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# for example ReprMessage -> ImmediateSyftMessageWithoutReply.signed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# == SignedImmediateSyftMessageWithoutReply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         return self.signed_type(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mmsg_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, address, obj_type, signature, verify_key, message, msg_id)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mmsg_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     ) -> None:\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, address, msg_id)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigning_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSigningKey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSignedMessageT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36mpost_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"signed\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0minit_reason\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" Signed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"> {init_reason} {self.pprint} {self.id.emoji()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36mpprint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{self.icon} ({self.class_name})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/SyftEnv/lib/python3.9/site-packages/syft/core/common/message.py\u001b[0m in \u001b[0;36mclass_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msigned_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSignedMessageT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631695917428
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "syftenv",
      "language": "python",
      "display_name": "syft"
    },
    "kernel_info": {
      "name": "syftenv"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "870a257bdc5888c6c86cab99e489c6f2724cdfe1684d5990b9d7489b2d46bf07"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}